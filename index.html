<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="SCANnotate++: CAD Annotations for ScanNet++">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SCANnotate++</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>


<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SCANnotate++: CAD Annotations ScanNet++</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://yuchenrao.github.io/" target="_blank">Yuchen Rao</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=VWNkvIwAAAAJ&hl=en&oi=ao" target="_blank">Stefan Ainetter</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://vevenom.github.io" target="_blank">Sinisa Stekovic</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://vincentlepetit.github.io/" target="_blank">Vincent Lepetit</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://www.tugraz.at/institute/icg/research/team-fraundorfer/people/friedrich-fraundorfer/" target="_blank">Friedrich Fraundorfer</a><sup>1</sup></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Institute of Visual Computing, Graz University of Technology, Austria</span><br>
            <span class="author-block"><sup>2</sup>LIGM, Ã‰cole des Ponts, IP Paris, CNRS, France</span>
          </div>
        </div>

        <div class="publication-links" style="margin-top: 1rem;">
          <!-- Paper Link -->
          <span class="link-block">
            <a href="https://www.arxiv.org/abs/2504.13580" target="_blank"
               class="external-link button is-normal is-rounded is-dark">
              <span class="icon"><i class="ai ai-arxiv"></i></span>
              <span>Paper</span>
            </a>
          </span>

          <!-- SCANnotate++ GitHub -->
          <span class="link-block">
            <a href="https://github.com/stefan-ainetter/SCANnotatepp/tree/main" target="_blank"
               class="external-link button is-normal is-rounded is-dark">
              <span class="icon"><i class="fab fa-github"></i></span>
              <span>SCANnotate++ Code</span>
            </a>
          </span>

          <!-- HOC-Search GitHub -->
          <span class="link-block">
            <a href="https://github.com/stefan-ainetter/HOC-Search" target="_blank"
               class="external-link button is-normal is-rounded is-dark">
              <span class="icon"><i class="fab fa-github"></i></span>
              <span>HOC-Search Code</span>
            </a>
          </span>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        We provide high-quality CAD model and pose annotations for the ScanNet++ dataset.
      </h2>
      <img src="./static/images/teaser.png" class="interpolation-image" alt="Teaser figure">
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <p>
          High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we will release our annotations, which we call SCANnotate++, along with our trained models.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <img src="./static/images/overview.png" class="interpolation-image" alt="Method overview">
        <p>
            Our pipeline begins with an RGB-D scan and the corresponding 3D instance segmentation labels. We extract bounding boxes
            for each segmented object and use them to initialize coarse object poses.
          </p>
          <p>
            We then perform <strong>CAD model retrieval and pose estimation</strong> using <a href="https://github.com/stefan-ainetter/HOC-Search" target="_blank">HOC-Search</a>, an algorithm that conducts joint discrete and continuous optimization
            over shape and pose parameters. It searches a tree-structured index of CAD models (the <em>HOC-Tree</em>) built from ShapeNet
            using shape similarity clustering, and iteratively refines pose using a differentiable render-and-compare loss.
          </p>
          <p>
            The objective function used during optimization combines <em>depth similarity</em>, <em>silhouette overlap</em>, and <em>chamfer distance</em>
            between the observed data and rendered CAD models. Gradient-based refinement improves pose alignment after each search iteration.
          </p>
          <p>
            To increase annotation consistency, we include a <strong>clustering and cloning stage</strong> where objects of the same class and
            similar shape within a scene are assigned the same CAD model. Final poses for cloned instances are then re-optimized.
          </p>
          <p>
            Finally, all annotations undergo <strong>manual quality verification</strong>. Erroneous alignments are corrected and the pose refinement
            is re-run to ensure high fidelity of the final SCANnotate++ dataset.
          </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments and Results</h2>
        <img src="./static/images/experiments.png" class="interpolation-image" alt="Experiments">
        <p>
          We evaluate SCANnotate++ annotations on two tasks: supervised point cloud completion and CAD model retrieval and alignment. Models trained on our annotations outperform those trained on manual annotations from Scan2CAD, validating the quality of our dataset.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{rao2025leveraging,
  author    = {Rao, Yuchen and Ainetter, Stefan and Stekovic, Sinisa and Lepetit, Vincent and Fraundorfer, Friedrich},
  title     = {Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding},
  journal   = {arXiv preprint arXiv:2504.13580},
  year      = {2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is based on template from <a
              href="https://nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>