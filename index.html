<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="SCANnotate++">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SCANnotate++</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>


<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">The SCANnotate++ Dataset</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://yuchenrao.github.io/" target="_blank">Yuchen Rao</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=VWNkvIwAAAAJ&hl=en&oi=ao" target="_blank">Stefan Ainetter</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://vevenom.github.io" target="_blank">Sinisa Stekovic</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://vincentlepetit.github.io/" target="_blank">Vincent Lepetit</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://www.tugraz.at/institute/icg/research/team-fraundorfer/people/friedrich-fraundorfer/" target="_blank">Friedrich Fraundorfer</a><sup>1</sup></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Institute of Visual Computing, Graz University of Technology, Austria</span><br>
            <span class="author-block"><sup>2</sup>LIGM, Ã‰cole des Ponts, IP Paris, CNRS, France</span>
          </div>

          <div class="publication-links" style="margin-top: 1rem;">
            <!-- Paper Link -->
            <span class="link-block">
              <a href="https://www.arxiv.org/abs/2504.13580" target="_blank"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>Paper</span>
              </a>
            </span>

            <!-- SCANnotate++ GitHub -->
            <span class="link-block">
              <a href="https://github.com/stefan-ainetter/SCANnotatepp/tree/main" target="_blank"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>SCANnotate++ Dataset</span>
              </a>
            </span>

            <!-- HOC-Search GitHub -->
            <span class="link-block">
              <a href="https://github.com/stefan-ainetter/HOC-Search" target="_blank"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>HOC-Search Code</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        We provide high-quality CAD model and pose annotations for the ScanNet++ v1 dataset created with our SCANnotate++ method.
      </h2>
      <img src="./static/images/teaser.png" class="interpolation-image" alt="Teaser figure">
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we release our annotations, which we call SCANnotate++, along with our trained models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <img src="./static/images/overview.png" class="interpolation-image" alt="Method overview">
          <p>
              Our pipeline begins with an RGB-D scan and the corresponding 3D instance segmentation labels. We extract bounding boxes
              for each segmented object and use them to initialize coarse object poses.
            </p>
            <p>
              We then perform <strong>CAD model retrieval and pose estimation</strong> using <a href="https://github.com/stefan-ainetter/HOC-Search" target="_blank">HOC-Search</a>, an algorithm that conducts joint discrete and continuous optimization
              over shape and pose parameters. It searches a tree-structured index of CAD models (the <em>HOC-Tree</em>) built from ShapeNet
              using shape similarity clustering, and iteratively refines pose using a differentiable render-and-compare loss.
            </p>
            <p>
              The objective function used during optimization combines <em>depth similarity</em>, <em>silhouette overlap</em>, and <em>chamfer distance</em>
              between the observed data and rendered CAD models. Gradient-based refinement improves pose alignment after each search iteration.
            </p>
            <p>
              To increase annotation consistency, we include a <strong>clustering and cloning stage</strong> where objects of the same class and
              similar shape within a scene are assigned the same CAD model. Final poses for cloned instances are then re-optimized.
            </p>
            <p>
              Finally, all annotations undergo <strong>manual quality verification</strong>. Erroneous alignments are corrected and the pose refinement
              is re-run to ensure high fidelity of the final SCANnotate++ dataset.
            </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Evaluation of the Annotations Quality</h2>
          <!-- <img src="./static/images/experiments.png" class="interpolation-image" alt="Experiments"> -->
          <div class="content has-text-justified">
            <p>
              We evaluate SCANnotate++ annotations on two tasks: supervised point cloud completion and CAD model retrieval and alignment. Models trained on our annotations outperform those trained on manual annotations from Scan2CAD, validating the quality of our dataset.
              The training and testing code will be made publicly available shortly.
            </p>
          </div>
          <h3 class="title is-4">Point Cloud Completion</h3>
          <div class="content has-text-justified">
            <img src="./static/images/ptcomplet.png" class="interpolation-pdf" alt="Point Cloud completion">
            <p>
              Given an input partial point cloud of objects in ScanNet and ScanNet++, we want to reconstruct the missing 3D points. We use automatic annotations from SCANnotate and SCANnotate++ to generate complete point clouds as ground truth for supervised learning of point cloud completion and present our visual results in the above figure.
              Top row shows partial point cloud input, middle row shows the reconstructed point cloud, and bottom row shows the ground
truth point cloud. 
            </p>
          </div>
          <h3 class="title is-4">CAD Model Retrieval and Alignment</h3>
          <div class="content has-text-justified">
            <img src="./static/images/roca.png" class="interpolation-image" alt="CAD Model Retrieval and Alignment">
            <p>
              Given a single image, we want to retrieve and align 3D CAD models, across the target categories. <a href="https://github.com/cangumeli/ROCA?tab=readme-ov-file" target="_blank">ROCA</a> is a framework that, from a single image, retrieves and aligns CAD models from a pool of per-scene objects. 
              We train ROCA using both SCANnotate and SCANnotate++, and present the visual results in the figure above. 
            </p>
          </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{rao2025leveraging,
  author    = {Rao, Yuchen and Ainetter, Stefan and Stekovic, Sinisa and Lepetit, Vincent and Fraundorfer, Friedrich},
  title     = {Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding},
  journal   = {arXiv preprint arXiv:2504.13580},
  year      = {2025}
}</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgments">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgments</h2>
    This project was funded in part by the European Union (ERC Advanced Grant explorer Funding ID #101097259).
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is based on template from <a
              href="https://nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
